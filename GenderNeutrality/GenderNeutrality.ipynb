{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenderNeutrality.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiAFzXkDSUvIEqLpoM3B6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohith4743/machinelearning/blob/main/GenderNeutrality/GenderNeutrality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSzxRHUei34a",
        "outputId": "4a1ccee6-4073-4bd3-959f-b38f63310284"
      },
      "source": [
        "!git clone https://github.com/rohith4743/machinelearning.git\n",
        "!mv machinelearning/GenderNeutrality/0753aae4bec411eb.zip 0753aae4bec411eb.zip\n",
        "!rm -r machinelearning\n",
        "!unzip 0753aae4bec411eb.zip\n",
        "!rm 0753aae4bec411eb.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machinelearning'...\n",
            "remote: Enumerating objects: 2050, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 2050 (delta 0), reused 0 (delta 0), pack-reused 2042\u001b[K\n",
            "Receiving objects: 100% (2050/2050), 107.27 MiB | 32.91 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Archive:  0753aae4bec411eb.zip\n",
            "  inflating: sample submission.csv   \n",
            "  inflating: Test.csv                \n",
            "  inflating: Train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEUVVnmTj7Jg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "feUfgVQ7dk3l",
        "outputId": "9e9f7947-900e-4a12-fc76-c5ac6a8a449f"
      },
      "source": [
        "train_csv = pd.read_csv(\"Train.csv\")\n",
        "train_csv.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EmpID</th>\n",
              "      <th>EmpName</th>\n",
              "      <th>LanguageOfCommunication</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>JobProfileIDApplyingFor</th>\n",
              "      <th>HighestDegree</th>\n",
              "      <th>DegreeBranch</th>\n",
              "      <th>GraduatingInstitute</th>\n",
              "      <th>LatestDegreeCGPA</th>\n",
              "      <th>YearsOfExperince</th>\n",
              "      <th>GraduationYear</th>\n",
              "      <th>CurrentCTC</th>\n",
              "      <th>ExpectedCTC</th>\n",
              "      <th>MartialStatus</th>\n",
              "      <th>EmpScore</th>\n",
              "      <th>CurrentDesignation</th>\n",
              "      <th>CurrentCompanyType</th>\n",
              "      <th>DepartmentInCompany</th>\n",
              "      <th>TotalLeavesTaken</th>\n",
              "      <th>BiasInfluentialFactor</th>\n",
              "      <th>FitmentPercent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11041</td>\n",
              "      <td>John</td>\n",
              "      <td>English</td>\n",
              "      <td>35</td>\n",
              "      <td>Male</td>\n",
              "      <td>JR85289</td>\n",
              "      <td>B.Tech</td>\n",
              "      <td>Electrical</td>\n",
              "      <td>Tier 1</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>2009</td>\n",
              "      <td>21</td>\n",
              "      <td>26</td>\n",
              "      <td>Married</td>\n",
              "      <td>5</td>\n",
              "      <td>SSE</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>Design</td>\n",
              "      <td>20</td>\n",
              "      <td>YearsOfExperince</td>\n",
              "      <td>95.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15079</td>\n",
              "      <td>William</td>\n",
              "      <td>English</td>\n",
              "      <td>26</td>\n",
              "      <td>Male</td>\n",
              "      <td>JR87525</td>\n",
              "      <td>B.Tech</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "      <td>Tier 3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2018</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>Married</td>\n",
              "      <td>5</td>\n",
              "      <td>BA</td>\n",
              "      <td>MidSized</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>67.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18638</td>\n",
              "      <td>James</td>\n",
              "      <td>English</td>\n",
              "      <td>36</td>\n",
              "      <td>Female</td>\n",
              "      <td>JR87525</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Tier 1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "      <td>15</td>\n",
              "      <td>24</td>\n",
              "      <td>Single</td>\n",
              "      <td>5</td>\n",
              "      <td>SDE</td>\n",
              "      <td>MidSized</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>19</td>\n",
              "      <td>Gender</td>\n",
              "      <td>91.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3941</td>\n",
              "      <td>Charles</td>\n",
              "      <td>English</td>\n",
              "      <td>29</td>\n",
              "      <td>Female</td>\n",
              "      <td>JR87525</td>\n",
              "      <td>BCA</td>\n",
              "      <td>Information Technology</td>\n",
              "      <td>Tier 2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "      <td>16</td>\n",
              "      <td>24</td>\n",
              "      <td>Married</td>\n",
              "      <td>5</td>\n",
              "      <td>SDE</td>\n",
              "      <td>Startup</td>\n",
              "      <td>Product</td>\n",
              "      <td>16</td>\n",
              "      <td>Gender</td>\n",
              "      <td>72.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5936</td>\n",
              "      <td>George</td>\n",
              "      <td>English</td>\n",
              "      <td>25</td>\n",
              "      <td>Male</td>\n",
              "      <td>JR70175</td>\n",
              "      <td>Dual M.Tech</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Tier 3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2019</td>\n",
              "      <td>24</td>\n",
              "      <td>32</td>\n",
              "      <td>Married</td>\n",
              "      <td>5</td>\n",
              "      <td>SDE</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>10</td>\n",
              "      <td>DegreeBranch</td>\n",
              "      <td>86.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   EmpID  EmpName  ... BiasInfluentialFactor  FitmentPercent\n",
              "0  11041     John  ...      YearsOfExperince           95.40\n",
              "1  15079  William  ...                   NaN           67.09\n",
              "2  18638    James  ...                Gender           91.26\n",
              "3   3941  Charles  ...                Gender           72.29\n",
              "4   5936   George  ...          DegreeBranch           86.34\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWgq_BqodxF-",
        "outputId": "0b9348b8-4645-4d7c-ffac-da2cd48f4008"
      },
      "source": [
        "dataset = train_csv.copy()\n",
        "dataset.pop(\"EmpName\")\n",
        "dataset.pop(\"EmpID\")\n",
        "dataset = pd.get_dummies(dataset, columns=[\"LanguageOfCommunication\" , \"Gender\", \"JobProfileIDApplyingFor\", \"HighestDegree\", \"DegreeBranch\", \"GraduatingInstitute\", \"MartialStatus\", \"CurrentDesignation\", \"CurrentCompanyType\", \"DepartmentInCompany\"])\n",
        "dataset.dtypes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Age                                          int64\n",
              "LatestDegreeCGPA                             int64\n",
              "YearsOfExperince                             int64\n",
              "GraduationYear                               int64\n",
              "CurrentCTC                                   int64\n",
              "ExpectedCTC                                  int64\n",
              "EmpScore                                     int64\n",
              "TotalLeavesTaken                             int64\n",
              "BiasInfluentialFactor                       object\n",
              "FitmentPercent                             float64\n",
              "LanguageOfCommunication_English              uint8\n",
              "LanguageOfCommunication_Hindi                uint8\n",
              "LanguageOfCommunication_Native               uint8\n",
              "Gender_Female                                uint8\n",
              "Gender_Male                                  uint8\n",
              "Gender_Other                                 uint8\n",
              "JobProfileIDApplyingFor_JR70175              uint8\n",
              "JobProfileIDApplyingFor_JR79193              uint8\n",
              "JobProfileIDApplyingFor_JR81165              uint8\n",
              "JobProfileIDApplyingFor_JR85289              uint8\n",
              "JobProfileIDApplyingFor_JR87525              uint8\n",
              "JobProfileIDApplyingFor_JR88654              uint8\n",
              "JobProfileIDApplyingFor_JR88873              uint8\n",
              "JobProfileIDApplyingFor_JR88879              uint8\n",
              "JobProfileIDApplyingFor_JR89890              uint8\n",
              "HighestDegree_B.Tech                         uint8\n",
              "HighestDegree_BCA                            uint8\n",
              "HighestDegree_Dual M.Tech                    uint8\n",
              "HighestDegree_Dual MBA                       uint8\n",
              "HighestDegree_M.Tech                         uint8\n",
              "HighestDegree_MCA                            uint8\n",
              "HighestDegree_MS                             uint8\n",
              "HighestDegree_PhD                            uint8\n",
              "DegreeBranch_Artificial Intelligence         uint8\n",
              "DegreeBranch_Computer Science                uint8\n",
              "DegreeBranch_Electrical                      uint8\n",
              "DegreeBranch_Electrical and Electronics      uint8\n",
              "DegreeBranch_Electronics                     uint8\n",
              "DegreeBranch_Information Technology          uint8\n",
              "GraduatingInstitute_Tier 1                   uint8\n",
              "GraduatingInstitute_Tier 2                   uint8\n",
              "GraduatingInstitute_Tier 3                   uint8\n",
              "MartialStatus_Married                        uint8\n",
              "MartialStatus_Single                         uint8\n",
              "CurrentDesignation_BA                        uint8\n",
              "CurrentDesignation_DA                        uint8\n",
              "CurrentDesignation_DE                        uint8\n",
              "CurrentDesignation_DS                        uint8\n",
              "CurrentDesignation_EM                        uint8\n",
              "CurrentDesignation_SDE                       uint8\n",
              "CurrentDesignation_SEM                       uint8\n",
              "CurrentDesignation_SSE                       uint8\n",
              "CurrentCompanyType_Enterprise                uint8\n",
              "CurrentCompanyType_MidSized                  uint8\n",
              "CurrentCompanyType_Startup                   uint8\n",
              "DepartmentInCompany_Customer Success         uint8\n",
              "DepartmentInCompany_Design                   uint8\n",
              "DepartmentInCompany_Engineering              uint8\n",
              "DepartmentInCompany_Finance                  uint8\n",
              "DepartmentInCompany_Product                  uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gqrq35OfBpy"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft4y5rkhgBBS"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels_Filment = train_features.pop('FitmentPercent')\n",
        "test_labels_Filment  = test_features.pop('FitmentPercent')\n",
        "\n",
        "train_labels_BiasInfluence = train_features.pop('BiasInfluentialFactor')\n",
        "test_labels_BiasInfluence = test_features.pop('BiasInfluentialFactor')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFzcBI3hDhF"
      },
      "source": [
        "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "normalizer.adapt(train_features)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQnfXjqShlZW"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 500])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g5kPAP5hue8",
        "outputId": "a44bad4f-6cb6-40ab-96ae-f6655731f860"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "      normalizer,\n",
        "      tf.keras.layers.Dense(1024, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(1024, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.7),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "model.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 58)                117       \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 1024)              60416     \n",
            "_________________________________________________________________\n",
            "dropout_99 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_100 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_101 (Dropout)        (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_102 (Dropout)        (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,979,894\n",
            "Trainable params: 1,979,777\n",
            "Non-trainable params: 117\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqlg0235iC3r",
        "outputId": "56ecdca0-d4c7-4097-8cdf-6ee0437fda0a"
      },
      "source": [
        "history = model.fit(train_features, train_labels_Filment, validation_split=0.2, verbose=1, epochs=1000, batch_size=128,  steps_per_epoch= (len(train_features) * 0.8) // 128)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 1701.5995 - val_loss: 2006.6802\n",
            "Epoch 2/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 869.0485 - val_loss: 2211.8491\n",
            "Epoch 3/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 822.9266 - val_loss: 2782.2029\n",
            "Epoch 4/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 785.2739 - val_loss: 2644.4136\n",
            "Epoch 5/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 781.1359 - val_loss: 2658.7542\n",
            "Epoch 6/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 746.1711 - val_loss: 2891.1443\n",
            "Epoch 7/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 711.9532 - val_loss: 2556.8694\n",
            "Epoch 8/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 702.8246 - val_loss: 2821.8320\n",
            "Epoch 9/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 684.5770 - val_loss: 2484.4688\n",
            "Epoch 10/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 688.0242 - val_loss: 2953.4443\n",
            "Epoch 11/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 672.7393 - val_loss: 2708.1919\n",
            "Epoch 12/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 663.9155 - val_loss: 2711.0210\n",
            "Epoch 13/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 639.2245 - val_loss: 2657.3899\n",
            "Epoch 14/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 645.2565 - val_loss: 2548.3291\n",
            "Epoch 15/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 635.0645 - val_loss: 2688.8926\n",
            "Epoch 16/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 636.0726 - val_loss: 2504.2881\n",
            "Epoch 17/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 627.4756 - val_loss: 2251.0835\n",
            "Epoch 18/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 615.4656 - val_loss: 2562.9253\n",
            "Epoch 19/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 605.1410 - val_loss: 2193.3281\n",
            "Epoch 20/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 588.2731 - val_loss: 2416.8813\n",
            "Epoch 21/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 576.8057 - val_loss: 2189.9307\n",
            "Epoch 22/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 591.7122 - val_loss: 2166.4998\n",
            "Epoch 23/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 563.0050 - val_loss: 1941.7887\n",
            "Epoch 24/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 549.1545 - val_loss: 1686.3392\n",
            "Epoch 25/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 568.8474 - val_loss: 1595.7271\n",
            "Epoch 26/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 550.2623 - val_loss: 1487.9801\n",
            "Epoch 27/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 527.6588 - val_loss: 1248.8896\n",
            "Epoch 28/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 517.2418 - val_loss: 1045.9197\n",
            "Epoch 29/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 526.5150 - val_loss: 1080.2351\n",
            "Epoch 30/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 507.2664 - val_loss: 954.7527\n",
            "Epoch 31/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 501.2105 - val_loss: 914.4893\n",
            "Epoch 32/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 494.2494 - val_loss: 651.9232\n",
            "Epoch 33/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 475.4558 - val_loss: 697.7129\n",
            "Epoch 34/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 470.9681 - val_loss: 617.4504\n",
            "Epoch 35/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 487.0103 - val_loss: 537.8380\n",
            "Epoch 36/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 462.5973 - val_loss: 479.9525\n",
            "Epoch 37/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 470.2365 - val_loss: 412.0423\n",
            "Epoch 38/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 460.8862 - val_loss: 407.5035\n",
            "Epoch 39/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 447.5799 - val_loss: 384.2651\n",
            "Epoch 40/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 447.3344 - val_loss: 269.9632\n",
            "Epoch 41/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 445.8340 - val_loss: 236.5967\n",
            "Epoch 42/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 434.4937 - val_loss: 183.8713\n",
            "Epoch 43/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 430.4032 - val_loss: 160.6298\n",
            "Epoch 44/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 412.0548 - val_loss: 134.3620\n",
            "Epoch 45/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 403.8774 - val_loss: 161.9235\n",
            "Epoch 46/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 399.3412 - val_loss: 146.5822\n",
            "Epoch 47/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 396.2651 - val_loss: 149.7321\n",
            "Epoch 48/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 384.9070 - val_loss: 151.6224\n",
            "Epoch 49/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 385.6943 - val_loss: 136.8981\n",
            "Epoch 50/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 375.0527 - val_loss: 124.0588\n",
            "Epoch 51/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 370.3507 - val_loss: 120.3980\n",
            "Epoch 52/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 364.1033 - val_loss: 112.4302\n",
            "Epoch 53/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 365.4404 - val_loss: 108.8690\n",
            "Epoch 54/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 369.1557 - val_loss: 106.9842\n",
            "Epoch 55/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 358.8944 - val_loss: 105.9886\n",
            "Epoch 56/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 358.0615 - val_loss: 110.7554\n",
            "Epoch 57/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 342.8735 - val_loss: 103.2629\n",
            "Epoch 58/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 343.5232 - val_loss: 107.6687\n",
            "Epoch 59/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 338.0995 - val_loss: 103.5893\n",
            "Epoch 60/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 344.1779 - val_loss: 108.1450\n",
            "Epoch 61/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 342.6222 - val_loss: 104.9088\n",
            "Epoch 62/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 339.5473 - val_loss: 108.6138\n",
            "Epoch 63/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 334.5465 - val_loss: 102.1059\n",
            "Epoch 64/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 336.8170 - val_loss: 105.8249\n",
            "Epoch 65/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 334.0454 - val_loss: 100.5527\n",
            "Epoch 66/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 333.8723 - val_loss: 107.3784\n",
            "Epoch 67/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 329.3414 - val_loss: 107.1978\n",
            "Epoch 68/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 332.8646 - val_loss: 104.4389\n",
            "Epoch 69/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 328.6638 - val_loss: 100.4661\n",
            "Epoch 70/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 319.9465 - val_loss: 102.5165\n",
            "Epoch 71/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 323.6075 - val_loss: 103.8258\n",
            "Epoch 72/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 320.1493 - val_loss: 108.6258\n",
            "Epoch 73/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 328.1425 - val_loss: 100.5993\n",
            "Epoch 74/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 318.1617 - val_loss: 102.6715\n",
            "Epoch 75/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 319.6017 - val_loss: 107.3241\n",
            "Epoch 76/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 321.0575 - val_loss: 101.5317\n",
            "Epoch 77/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 320.5307 - val_loss: 107.0284\n",
            "Epoch 78/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 302.8849 - val_loss: 111.7443\n",
            "Epoch 79/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 314.3836 - val_loss: 102.8092\n",
            "Epoch 80/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 307.0645 - val_loss: 103.4447\n",
            "Epoch 81/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 299.9940 - val_loss: 106.2282\n",
            "Epoch 82/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 307.3805 - val_loss: 95.8619\n",
            "Epoch 83/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 298.0978 - val_loss: 125.4004\n",
            "Epoch 84/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 307.4327 - val_loss: 98.4145\n",
            "Epoch 85/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 303.7509 - val_loss: 107.4354\n",
            "Epoch 86/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 297.3957 - val_loss: 101.7530\n",
            "Epoch 87/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 305.7406 - val_loss: 97.8360\n",
            "Epoch 88/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 306.9958 - val_loss: 104.6075\n",
            "Epoch 89/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 296.1024 - val_loss: 98.3447\n",
            "Epoch 90/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 298.0552 - val_loss: 111.4815\n",
            "Epoch 91/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 291.4503 - val_loss: 103.1692\n",
            "Epoch 92/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 287.9778 - val_loss: 105.1493\n",
            "Epoch 93/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 296.0640 - val_loss: 111.4793\n",
            "Epoch 94/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 289.3310 - val_loss: 114.4157\n",
            "Epoch 95/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 286.6900 - val_loss: 108.4106\n",
            "Epoch 96/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 288.3198 - val_loss: 105.6291\n",
            "Epoch 97/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 286.5770 - val_loss: 105.6228\n",
            "Epoch 98/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 275.8069 - val_loss: 103.9606\n",
            "Epoch 99/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 279.2387 - val_loss: 109.2033\n",
            "Epoch 100/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 280.9846 - val_loss: 102.8495\n",
            "Epoch 101/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 281.5101 - val_loss: 100.4975\n",
            "Epoch 102/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 277.7812 - val_loss: 112.6140\n",
            "Epoch 103/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 276.1963 - val_loss: 109.6238\n",
            "Epoch 104/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 277.4164 - val_loss: 107.7464\n",
            "Epoch 105/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 279.3849 - val_loss: 121.7730\n",
            "Epoch 106/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 278.0868 - val_loss: 106.1586\n",
            "Epoch 107/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 276.9798 - val_loss: 97.0074\n",
            "Epoch 108/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 283.6554 - val_loss: 106.4792\n",
            "Epoch 109/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 270.9218 - val_loss: 104.5411\n",
            "Epoch 110/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 276.2339 - val_loss: 103.3700\n",
            "Epoch 111/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 272.7171 - val_loss: 105.5947\n",
            "Epoch 112/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 277.0247 - val_loss: 98.6263\n",
            "Epoch 113/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 277.4544 - val_loss: 102.0164\n",
            "Epoch 114/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 274.2897 - val_loss: 109.4330\n",
            "Epoch 115/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 265.8335 - val_loss: 104.6275\n",
            "Epoch 116/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 267.6422 - val_loss: 98.1577\n",
            "Epoch 117/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 259.3022 - val_loss: 100.3367\n",
            "Epoch 118/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 267.5448 - val_loss: 106.8415\n",
            "Epoch 119/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 267.6660 - val_loss: 114.3100\n",
            "Epoch 120/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 260.4442 - val_loss: 110.0638\n",
            "Epoch 121/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 258.7379 - val_loss: 106.8465\n",
            "Epoch 122/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 269.3251 - val_loss: 103.8095\n",
            "Epoch 123/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 266.2137 - val_loss: 99.9806\n",
            "Epoch 124/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 259.9669 - val_loss: 99.3884\n",
            "Epoch 125/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 260.7264 - val_loss: 105.7060\n",
            "Epoch 126/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 258.7056 - val_loss: 106.0504\n",
            "Epoch 127/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 257.0992 - val_loss: 105.6531\n",
            "Epoch 128/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 253.5278 - val_loss: 106.5619\n",
            "Epoch 129/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 253.9406 - val_loss: 103.6497\n",
            "Epoch 130/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 253.3932 - val_loss: 103.6525\n",
            "Epoch 131/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 249.4336 - val_loss: 110.3384\n",
            "Epoch 132/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 244.9347 - val_loss: 101.5226\n",
            "Epoch 133/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 242.2932 - val_loss: 97.3917\n",
            "Epoch 134/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 243.5410 - val_loss: 101.9990\n",
            "Epoch 135/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 248.4098 - val_loss: 100.2518\n",
            "Epoch 136/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 241.4317 - val_loss: 98.9729\n",
            "Epoch 137/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 241.3341 - val_loss: 100.9063\n",
            "Epoch 138/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 242.8203 - val_loss: 110.1186\n",
            "Epoch 139/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 241.0782 - val_loss: 102.6441\n",
            "Epoch 140/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 249.1028 - val_loss: 97.0976\n",
            "Epoch 141/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 240.2767 - val_loss: 97.5904\n",
            "Epoch 142/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 242.2384 - val_loss: 98.8864\n",
            "Epoch 143/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 238.1291 - val_loss: 97.0883\n",
            "Epoch 144/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 245.0629 - val_loss: 104.2778\n",
            "Epoch 145/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 243.7116 - val_loss: 110.5970\n",
            "Epoch 146/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 238.9627 - val_loss: 104.0886\n",
            "Epoch 147/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 239.7877 - val_loss: 97.8605\n",
            "Epoch 148/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 237.5327 - val_loss: 102.9242\n",
            "Epoch 149/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 237.1409 - val_loss: 101.5737\n",
            "Epoch 150/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 243.5385 - val_loss: 105.1267\n",
            "Epoch 151/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 243.1001 - val_loss: 103.4886\n",
            "Epoch 152/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 237.0389 - val_loss: 105.0374\n",
            "Epoch 153/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 234.6881 - val_loss: 106.1021\n",
            "Epoch 154/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 232.5666 - val_loss: 107.9286\n",
            "Epoch 155/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 240.3319 - val_loss: 103.3878\n",
            "Epoch 156/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 241.9013 - val_loss: 101.5527\n",
            "Epoch 157/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 234.5385 - val_loss: 111.2296\n",
            "Epoch 158/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 236.9377 - val_loss: 105.2168\n",
            "Epoch 159/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 237.0251 - val_loss: 96.6747\n",
            "Epoch 160/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 236.0194 - val_loss: 107.0796\n",
            "Epoch 161/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 231.6764 - val_loss: 100.3357\n",
            "Epoch 162/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 229.2277 - val_loss: 102.0956\n",
            "Epoch 163/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 234.1060 - val_loss: 112.3552\n",
            "Epoch 164/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 231.8363 - val_loss: 113.9453\n",
            "Epoch 165/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 229.7601 - val_loss: 100.8196\n",
            "Epoch 166/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 235.7663 - val_loss: 97.4942\n",
            "Epoch 167/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 232.5154 - val_loss: 98.6531\n",
            "Epoch 168/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 234.1442 - val_loss: 105.8704\n",
            "Epoch 169/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 231.1628 - val_loss: 99.4631\n",
            "Epoch 170/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.5230 - val_loss: 102.2467\n",
            "Epoch 171/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.8618 - val_loss: 100.9372\n",
            "Epoch 172/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 232.3461 - val_loss: 99.2376\n",
            "Epoch 173/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 230.2077 - val_loss: 103.2189\n",
            "Epoch 174/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 227.1141 - val_loss: 102.0436\n",
            "Epoch 175/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 230.8362 - val_loss: 102.0490\n",
            "Epoch 176/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 229.7947 - val_loss: 103.3658\n",
            "Epoch 177/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 228.3233 - val_loss: 103.8751\n",
            "Epoch 178/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 233.4328 - val_loss: 98.8613\n",
            "Epoch 179/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 229.5369 - val_loss: 96.6273\n",
            "Epoch 180/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 227.0637 - val_loss: 98.2308\n",
            "Epoch 181/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 226.6385 - val_loss: 103.3728\n",
            "Epoch 182/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 225.3143 - val_loss: 97.4407\n",
            "Epoch 183/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.5563 - val_loss: 106.9006\n",
            "Epoch 184/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 218.0531 - val_loss: 104.3361\n",
            "Epoch 185/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 226.2352 - val_loss: 96.1266\n",
            "Epoch 186/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 224.2943 - val_loss: 99.5846\n",
            "Epoch 187/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.8115 - val_loss: 103.0021\n",
            "Epoch 188/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 222.2630 - val_loss: 95.2889\n",
            "Epoch 189/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.7406 - val_loss: 100.6666\n",
            "Epoch 190/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 225.3167 - val_loss: 97.1901\n",
            "Epoch 191/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 229.3225 - val_loss: 101.0310\n",
            "Epoch 192/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 227.3024 - val_loss: 98.8390\n",
            "Epoch 193/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 226.4017 - val_loss: 98.8153\n",
            "Epoch 194/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 218.0105 - val_loss: 99.5828\n",
            "Epoch 195/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 221.1427 - val_loss: 99.0438\n",
            "Epoch 196/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 227.1296 - val_loss: 100.5602\n",
            "Epoch 197/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 219.4826 - val_loss: 108.0566\n",
            "Epoch 198/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.4170 - val_loss: 99.7412\n",
            "Epoch 199/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 225.4494 - val_loss: 103.3286\n",
            "Epoch 200/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 224.0490 - val_loss: 100.5937\n",
            "Epoch 201/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 217.9012 - val_loss: 97.3931\n",
            "Epoch 202/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 223.6279 - val_loss: 96.0498\n",
            "Epoch 203/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 219.8123 - val_loss: 102.2088\n",
            "Epoch 204/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 218.4296 - val_loss: 102.7418\n",
            "Epoch 205/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.5461 - val_loss: 102.1649\n",
            "Epoch 206/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 218.9768 - val_loss: 103.2134\n",
            "Epoch 207/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 228.9411 - val_loss: 99.1359\n",
            "Epoch 208/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 224.1645 - val_loss: 99.7310\n",
            "Epoch 209/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 228.3993 - val_loss: 104.5068\n",
            "Epoch 210/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 227.3047 - val_loss: 99.7146\n",
            "Epoch 211/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 222.2602 - val_loss: 100.1155\n",
            "Epoch 212/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 215.5793 - val_loss: 100.7839\n",
            "Epoch 213/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 222.6973 - val_loss: 96.5367\n",
            "Epoch 214/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 226.2250 - val_loss: 98.5975\n",
            "Epoch 215/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 221.9514 - val_loss: 98.8725\n",
            "Epoch 216/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.2228 - val_loss: 98.4008\n",
            "Epoch 217/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 219.5846 - val_loss: 96.1110\n",
            "Epoch 218/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 223.5658 - val_loss: 96.7792\n",
            "Epoch 219/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 221.1000 - val_loss: 93.7437\n",
            "Epoch 220/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 224.8056 - val_loss: 99.3753\n",
            "Epoch 221/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 221.3904 - val_loss: 100.7539\n",
            "Epoch 222/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 218.5201 - val_loss: 96.7849\n",
            "Epoch 223/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.6104 - val_loss: 98.9535\n",
            "Epoch 224/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 217.5733 - val_loss: 96.5599\n",
            "Epoch 225/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 222.9101 - val_loss: 96.6614\n",
            "Epoch 226/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 221.3560 - val_loss: 95.3389\n",
            "Epoch 227/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.3425 - val_loss: 100.8699\n",
            "Epoch 228/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 219.2247 - val_loss: 96.6108\n",
            "Epoch 229/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 215.9759 - val_loss: 96.2367\n",
            "Epoch 230/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 217.2706 - val_loss: 98.0066\n",
            "Epoch 231/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.8326 - val_loss: 100.9895\n",
            "Epoch 232/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 216.1989 - val_loss: 104.4234\n",
            "Epoch 233/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.0047 - val_loss: 101.0446\n",
            "Epoch 234/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 214.3143 - val_loss: 97.8176\n",
            "Epoch 235/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.6479 - val_loss: 97.7251\n",
            "Epoch 236/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.5319 - val_loss: 104.0246\n",
            "Epoch 237/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 218.6051 - val_loss: 104.0717\n",
            "Epoch 238/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 217.5566 - val_loss: 101.2443\n",
            "Epoch 239/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.8615 - val_loss: 100.2696\n",
            "Epoch 240/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 219.2480 - val_loss: 99.5562\n",
            "Epoch 241/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.9813 - val_loss: 104.6768\n",
            "Epoch 242/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 213.2775 - val_loss: 106.3541\n",
            "Epoch 243/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 214.0530 - val_loss: 99.5345\n",
            "Epoch 244/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 223.2142 - val_loss: 96.6144\n",
            "Epoch 245/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 216.8264 - val_loss: 102.5291\n",
            "Epoch 246/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 213.6507 - val_loss: 104.0385\n",
            "Epoch 247/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.2027 - val_loss: 100.5990\n",
            "Epoch 248/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 217.1684 - val_loss: 94.2589\n",
            "Epoch 249/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.9495 - val_loss: 103.7801\n",
            "Epoch 250/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 215.9270 - val_loss: 99.0561\n",
            "Epoch 251/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 213.5512 - val_loss: 102.4449\n",
            "Epoch 252/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.3595 - val_loss: 100.1997\n",
            "Epoch 253/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.0172 - val_loss: 100.0056\n",
            "Epoch 254/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.9406 - val_loss: 100.6776\n",
            "Epoch 255/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.1802 - val_loss: 101.6493\n",
            "Epoch 256/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 210.4719 - val_loss: 101.7903\n",
            "Epoch 257/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 211.8051 - val_loss: 101.1555\n",
            "Epoch 258/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 211.0579 - val_loss: 102.8248\n",
            "Epoch 259/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 210.1249 - val_loss: 100.2173\n",
            "Epoch 260/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 211.4452 - val_loss: 95.8390\n",
            "Epoch 261/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.3494 - val_loss: 99.4343\n",
            "Epoch 262/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.5584 - val_loss: 102.7514\n",
            "Epoch 263/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 211.1683 - val_loss: 101.6842\n",
            "Epoch 264/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.5252 - val_loss: 101.8850\n",
            "Epoch 265/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.0462 - val_loss: 97.1980\n",
            "Epoch 266/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 206.7858 - val_loss: 98.1107\n",
            "Epoch 267/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.0486 - val_loss: 103.9696\n",
            "Epoch 268/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.7629 - val_loss: 100.2939\n",
            "Epoch 269/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.8351 - val_loss: 99.8298\n",
            "Epoch 270/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.6534 - val_loss: 105.9742\n",
            "Epoch 271/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.6581 - val_loss: 100.7012\n",
            "Epoch 272/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.4945 - val_loss: 106.7979\n",
            "Epoch 273/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.8496 - val_loss: 100.0548\n",
            "Epoch 274/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 214.1693 - val_loss: 101.6991\n",
            "Epoch 275/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 208.4398 - val_loss: 102.4720\n",
            "Epoch 276/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.7412 - val_loss: 101.6490\n",
            "Epoch 277/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 207.8458 - val_loss: 98.6766\n",
            "Epoch 278/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.6678 - val_loss: 100.3832\n",
            "Epoch 279/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.3717 - val_loss: 96.8198\n",
            "Epoch 280/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.3980 - val_loss: 101.6985\n",
            "Epoch 281/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.4290 - val_loss: 100.4485\n",
            "Epoch 282/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 212.5453 - val_loss: 96.5731\n",
            "Epoch 283/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 209.1034 - val_loss: 103.0476\n",
            "Epoch 284/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 210.9619 - val_loss: 99.9350\n",
            "Epoch 285/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 205.9743 - val_loss: 100.9203\n",
            "Epoch 286/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 210.6429 - val_loss: 104.0761\n",
            "Epoch 287/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.3783 - val_loss: 101.4811\n",
            "Epoch 288/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.3035 - val_loss: 102.2296\n",
            "Epoch 289/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 207.8339 - val_loss: 99.8319\n",
            "Epoch 290/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 216.7444 - val_loss: 102.3602\n",
            "Epoch 291/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 202.7118 - val_loss: 100.1191\n",
            "Epoch 292/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.2050 - val_loss: 98.9518\n",
            "Epoch 293/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 214.1656 - val_loss: 101.3379\n",
            "Epoch 294/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 208.2502 - val_loss: 98.4475\n",
            "Epoch 295/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 203.0697 - val_loss: 105.0469\n",
            "Epoch 296/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 201.3982 - val_loss: 101.6537\n",
            "Epoch 297/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 208.8139 - val_loss: 96.8046\n",
            "Epoch 298/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.9706 - val_loss: 96.9094\n",
            "Epoch 299/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.8056 - val_loss: 96.3797\n",
            "Epoch 300/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 201.3678 - val_loss: 102.3986\n",
            "Epoch 301/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.6961 - val_loss: 97.5098\n",
            "Epoch 302/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 202.7709 - val_loss: 98.3894\n",
            "Epoch 303/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 207.0332 - val_loss: 97.9747\n",
            "Epoch 304/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 212.6420 - val_loss: 100.9728\n",
            "Epoch 305/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 202.6817 - val_loss: 98.8091\n",
            "Epoch 306/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.6040 - val_loss: 98.7335\n",
            "Epoch 307/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.8090 - val_loss: 97.0590\n",
            "Epoch 308/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.9671 - val_loss: 101.1952\n",
            "Epoch 309/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.0998 - val_loss: 97.6829\n",
            "Epoch 310/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 202.6962 - val_loss: 97.9619\n",
            "Epoch 311/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.8324 - val_loss: 108.0471\n",
            "Epoch 312/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.3203 - val_loss: 99.9067\n",
            "Epoch 313/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.9714 - val_loss: 100.7895\n",
            "Epoch 314/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.6671 - val_loss: 96.8783\n",
            "Epoch 315/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 203.8150 - val_loss: 97.3924\n",
            "Epoch 316/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 208.3131 - val_loss: 99.3614\n",
            "Epoch 317/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 204.3603 - val_loss: 100.8904\n",
            "Epoch 318/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.4886 - val_loss: 101.8766\n",
            "Epoch 319/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 208.7393 - val_loss: 104.1229\n",
            "Epoch 320/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 201.7590 - val_loss: 103.9741\n",
            "Epoch 321/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 210.3582 - val_loss: 103.0420\n",
            "Epoch 322/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 197.9613 - val_loss: 99.0734\n",
            "Epoch 323/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.5301 - val_loss: 96.3638\n",
            "Epoch 324/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 201.1993 - val_loss: 97.9462\n",
            "Epoch 325/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 202.9209 - val_loss: 97.6205\n",
            "Epoch 326/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 197.2344 - val_loss: 100.7620\n",
            "Epoch 327/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 201.1241 - val_loss: 100.6851\n",
            "Epoch 328/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.4438 - val_loss: 96.6690\n",
            "Epoch 329/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 203.0679 - val_loss: 96.1678\n",
            "Epoch 330/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 202.6556 - val_loss: 99.4077\n",
            "Epoch 331/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.6718 - val_loss: 98.3492\n",
            "Epoch 332/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 196.5517 - val_loss: 97.8759\n",
            "Epoch 333/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.4109 - val_loss: 99.7101\n",
            "Epoch 334/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 202.8490 - val_loss: 98.5301\n",
            "Epoch 335/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 213.5647 - val_loss: 99.4186\n",
            "Epoch 336/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.6891 - val_loss: 95.0583\n",
            "Epoch 337/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.5159 - val_loss: 97.0502\n",
            "Epoch 338/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.3710 - val_loss: 101.0568\n",
            "Epoch 339/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.8678 - val_loss: 101.1342\n",
            "Epoch 340/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 203.6670 - val_loss: 101.4487\n",
            "Epoch 341/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.0510 - val_loss: 99.3369\n",
            "Epoch 342/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.4188 - val_loss: 100.2524\n",
            "Epoch 343/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.8207 - val_loss: 99.0038\n",
            "Epoch 344/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.0403 - val_loss: 100.5345\n",
            "Epoch 345/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.5650 - val_loss: 107.2086\n",
            "Epoch 346/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.2712 - val_loss: 99.1617\n",
            "Epoch 347/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.1482 - val_loss: 96.9677\n",
            "Epoch 348/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.7433 - val_loss: 96.8438\n",
            "Epoch 349/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 193.6761 - val_loss: 96.9398\n",
            "Epoch 350/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 199.0042 - val_loss: 97.6986\n",
            "Epoch 351/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.6621 - val_loss: 97.7820\n",
            "Epoch 352/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.8966 - val_loss: 98.3148\n",
            "Epoch 353/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 199.6226 - val_loss: 96.2652\n",
            "Epoch 354/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.9625 - val_loss: 97.1553\n",
            "Epoch 355/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 196.3055 - val_loss: 96.9758\n",
            "Epoch 356/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 197.7377 - val_loss: 96.7140\n",
            "Epoch 357/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.3170 - val_loss: 98.6536\n",
            "Epoch 358/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 193.9055 - val_loss: 100.4427\n",
            "Epoch 359/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.5844 - val_loss: 106.2102\n",
            "Epoch 360/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.4793 - val_loss: 103.7213\n",
            "Epoch 361/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 197.7219 - val_loss: 102.8572\n",
            "Epoch 362/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 220.9222 - val_loss: 109.1846\n",
            "Epoch 363/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 200.4643 - val_loss: 99.6019\n",
            "Epoch 364/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.2042 - val_loss: 99.4491\n",
            "Epoch 365/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.0038 - val_loss: 97.3712\n",
            "Epoch 366/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.2962 - val_loss: 98.1391\n",
            "Epoch 367/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.3581 - val_loss: 101.5458\n",
            "Epoch 368/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.0344 - val_loss: 98.9247\n",
            "Epoch 369/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.3608 - val_loss: 100.0252\n",
            "Epoch 370/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 198.5496 - val_loss: 99.3486\n",
            "Epoch 371/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 197.0266 - val_loss: 98.9989\n",
            "Epoch 372/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 196.3394 - val_loss: 99.6310\n",
            "Epoch 373/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.0403 - val_loss: 95.8833\n",
            "Epoch 374/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.2490 - val_loss: 98.6063\n",
            "Epoch 375/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 191.0978 - val_loss: 95.7538\n",
            "Epoch 376/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.0542 - val_loss: 100.6548\n",
            "Epoch 377/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.8915 - val_loss: 100.5308\n",
            "Epoch 378/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 191.6190 - val_loss: 99.8009\n",
            "Epoch 379/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.0881 - val_loss: 99.6491\n",
            "Epoch 380/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 189.7578 - val_loss: 102.5255\n",
            "Epoch 381/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.8442 - val_loss: 100.8056\n",
            "Epoch 382/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.6342 - val_loss: 101.1137\n",
            "Epoch 383/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.8627 - val_loss: 100.9805\n",
            "Epoch 384/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 194.9557 - val_loss: 103.8555\n",
            "Epoch 385/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.7052 - val_loss: 102.5083\n",
            "Epoch 386/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 191.9639 - val_loss: 99.3428\n",
            "Epoch 387/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.6852 - val_loss: 100.0980\n",
            "Epoch 388/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.9595 - val_loss: 102.9935\n",
            "Epoch 389/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.5716 - val_loss: 102.7195\n",
            "Epoch 390/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 195.7811 - val_loss: 99.7621\n",
            "Epoch 391/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 187.5486 - val_loss: 100.5621\n",
            "Epoch 392/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 189.6613 - val_loss: 100.3397\n",
            "Epoch 393/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 188.1524 - val_loss: 103.6794\n",
            "Epoch 394/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 189.9811 - val_loss: 99.9738\n",
            "Epoch 395/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 187.3582 - val_loss: 98.8312\n",
            "Epoch 396/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 189.7259 - val_loss: 99.6121\n",
            "Epoch 397/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.9643 - val_loss: 94.4882\n",
            "Epoch 398/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.3047 - val_loss: 98.5168\n",
            "Epoch 399/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.5777 - val_loss: 98.3752\n",
            "Epoch 400/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.0328 - val_loss: 97.9623\n",
            "Epoch 401/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.5185 - val_loss: 97.2492\n",
            "Epoch 402/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.3914 - val_loss: 98.4472\n",
            "Epoch 403/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.5716 - val_loss: 97.1610\n",
            "Epoch 404/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.5225 - val_loss: 99.3115\n",
            "Epoch 405/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 191.1538 - val_loss: 99.9005\n",
            "Epoch 406/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.6616 - val_loss: 97.5404\n",
            "Epoch 407/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.7265 - val_loss: 97.4612\n",
            "Epoch 408/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.5732 - val_loss: 97.6728\n",
            "Epoch 409/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 196.4004 - val_loss: 100.0362\n",
            "Epoch 410/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.7421 - val_loss: 101.7544\n",
            "Epoch 411/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.7420 - val_loss: 98.8085\n",
            "Epoch 412/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 192.5726 - val_loss: 97.0757\n",
            "Epoch 413/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.1917 - val_loss: 100.2913\n",
            "Epoch 414/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.6067 - val_loss: 102.0259\n",
            "Epoch 415/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.8233 - val_loss: 101.8655\n",
            "Epoch 416/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.3316 - val_loss: 99.9526\n",
            "Epoch 417/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 187.0937 - val_loss: 105.1809\n",
            "Epoch 418/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.9617 - val_loss: 99.4448\n",
            "Epoch 419/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 188.4834 - val_loss: 98.4482\n",
            "Epoch 420/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.7995 - val_loss: 97.6348\n",
            "Epoch 421/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.3415 - val_loss: 98.9837\n",
            "Epoch 422/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 185.4660 - val_loss: 98.5630\n",
            "Epoch 423/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.9340 - val_loss: 97.5435\n",
            "Epoch 424/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.7906 - val_loss: 98.4500\n",
            "Epoch 425/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.1721 - val_loss: 100.8542\n",
            "Epoch 426/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 187.4410 - val_loss: 97.8545\n",
            "Epoch 427/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 184.7442 - val_loss: 101.5313\n",
            "Epoch 428/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 190.8808 - val_loss: 100.5533\n",
            "Epoch 429/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 183.3175 - val_loss: 98.5313\n",
            "Epoch 430/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 191.0947 - val_loss: 102.9337\n",
            "Epoch 431/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.6113 - val_loss: 100.3011\n",
            "Epoch 432/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.7603 - val_loss: 99.3413\n",
            "Epoch 433/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 184.5261 - val_loss: 100.2955\n",
            "Epoch 434/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 183.7386 - val_loss: 97.4570\n",
            "Epoch 435/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.2084 - val_loss: 97.2529\n",
            "Epoch 436/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 187.3990 - val_loss: 97.4904\n",
            "Epoch 437/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 187.1913 - val_loss: 98.9648\n",
            "Epoch 438/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.4358 - val_loss: 98.6641\n",
            "Epoch 439/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.0417 - val_loss: 100.8802\n",
            "Epoch 440/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.1400 - val_loss: 99.7115\n",
            "Epoch 441/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.0444 - val_loss: 102.0617\n",
            "Epoch 442/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.0902 - val_loss: 102.1502\n",
            "Epoch 443/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.1849 - val_loss: 104.4791\n",
            "Epoch 444/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 180.9156 - val_loss: 105.9149\n",
            "Epoch 445/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.7151 - val_loss: 99.3679\n",
            "Epoch 446/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.6923 - val_loss: 101.6897\n",
            "Epoch 447/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 184.8226 - val_loss: 98.6061\n",
            "Epoch 448/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 188.1624 - val_loss: 101.5685\n",
            "Epoch 449/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 183.3042 - val_loss: 97.6570\n",
            "Epoch 450/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.0095 - val_loss: 95.9296\n",
            "Epoch 451/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.2389 - val_loss: 96.0834\n",
            "Epoch 452/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.6699 - val_loss: 97.8460\n",
            "Epoch 453/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 176.8435 - val_loss: 101.5764\n",
            "Epoch 454/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.4205 - val_loss: 99.2767\n",
            "Epoch 455/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 176.8782 - val_loss: 98.2143\n",
            "Epoch 456/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.5114 - val_loss: 100.1328\n",
            "Epoch 457/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.7386 - val_loss: 101.8555\n",
            "Epoch 458/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.0392 - val_loss: 99.5128\n",
            "Epoch 459/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.8547 - val_loss: 98.1993\n",
            "Epoch 460/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.9575 - val_loss: 100.1584\n",
            "Epoch 461/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.5396 - val_loss: 99.0161\n",
            "Epoch 462/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.3448 - val_loss: 98.0011\n",
            "Epoch 463/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.9575 - val_loss: 99.6455\n",
            "Epoch 464/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 183.5121 - val_loss: 98.9388\n",
            "Epoch 465/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.9298 - val_loss: 101.6552\n",
            "Epoch 466/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 179.2141 - val_loss: 99.2134\n",
            "Epoch 467/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 185.1279 - val_loss: 99.3763\n",
            "Epoch 468/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.2391 - val_loss: 100.2432\n",
            "Epoch 469/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.3593 - val_loss: 98.9908\n",
            "Epoch 470/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.7094 - val_loss: 100.6467\n",
            "Epoch 471/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 177.1373 - val_loss: 107.3829\n",
            "Epoch 472/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 174.8063 - val_loss: 100.1552\n",
            "Epoch 473/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.2290 - val_loss: 99.7455\n",
            "Epoch 474/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.1973 - val_loss: 102.6715\n",
            "Epoch 475/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.9916 - val_loss: 101.3352\n",
            "Epoch 476/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 184.9761 - val_loss: 107.3815\n",
            "Epoch 477/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 205.6436 - val_loss: 106.3375\n",
            "Epoch 478/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.9490 - val_loss: 99.3281\n",
            "Epoch 479/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.2277 - val_loss: 98.2163\n",
            "Epoch 480/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.5622 - val_loss: 99.4074\n",
            "Epoch 481/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 176.3451 - val_loss: 95.0745\n",
            "Epoch 482/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.3828 - val_loss: 95.7137\n",
            "Epoch 483/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.5821 - val_loss: 97.2757\n",
            "Epoch 484/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.5857 - val_loss: 101.0606\n",
            "Epoch 485/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.3429 - val_loss: 99.3461\n",
            "Epoch 486/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.2975 - val_loss: 97.4095\n",
            "Epoch 487/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 186.2034 - val_loss: 98.4634\n",
            "Epoch 488/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.4945 - val_loss: 95.4914\n",
            "Epoch 489/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.5748 - val_loss: 103.3700\n",
            "Epoch 490/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 182.6343 - val_loss: 98.4883\n",
            "Epoch 491/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.2800 - val_loss: 98.9999\n",
            "Epoch 492/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.9220 - val_loss: 98.7247\n",
            "Epoch 493/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.9966 - val_loss: 103.0875\n",
            "Epoch 494/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 183.9676 - val_loss: 100.7262\n",
            "Epoch 495/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 176.1078 - val_loss: 100.4860\n",
            "Epoch 496/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.5372 - val_loss: 97.4789\n",
            "Epoch 497/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 178.3560 - val_loss: 97.5867\n",
            "Epoch 498/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.1350 - val_loss: 100.7862\n",
            "Epoch 499/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.6774 - val_loss: 99.6943\n",
            "Epoch 500/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 172.5677 - val_loss: 100.3557\n",
            "Epoch 501/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.8295 - val_loss: 100.0518\n",
            "Epoch 502/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 173.4963 - val_loss: 98.6774\n",
            "Epoch 503/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.3406 - val_loss: 101.8500\n",
            "Epoch 504/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.8324 - val_loss: 102.5349\n",
            "Epoch 505/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.9590 - val_loss: 102.6905\n",
            "Epoch 506/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 174.3435 - val_loss: 98.7140\n",
            "Epoch 507/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.6121 - val_loss: 100.5776\n",
            "Epoch 508/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.5868 - val_loss: 100.7525\n",
            "Epoch 509/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.2455 - val_loss: 97.9897\n",
            "Epoch 510/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.3443 - val_loss: 100.1782\n",
            "Epoch 511/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.7601 - val_loss: 98.9818\n",
            "Epoch 512/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.4737 - val_loss: 96.3747\n",
            "Epoch 513/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 179.3014 - val_loss: 98.4523\n",
            "Epoch 514/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.1255 - val_loss: 103.8659\n",
            "Epoch 515/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.8784 - val_loss: 101.5760\n",
            "Epoch 516/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.5209 - val_loss: 99.8540\n",
            "Epoch 517/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.9114 - val_loss: 102.7747\n",
            "Epoch 518/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.9531 - val_loss: 96.4145\n",
            "Epoch 519/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.9138 - val_loss: 98.3666\n",
            "Epoch 520/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 181.3903 - val_loss: 98.6168\n",
            "Epoch 521/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.1555 - val_loss: 97.0057\n",
            "Epoch 522/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 171.5253 - val_loss: 97.1104\n",
            "Epoch 523/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.1610 - val_loss: 96.9281\n",
            "Epoch 524/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 174.1915 - val_loss: 100.1701\n",
            "Epoch 525/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.3101 - val_loss: 100.5120\n",
            "Epoch 526/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 174.2397 - val_loss: 99.5628\n",
            "Epoch 527/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.5393 - val_loss: 98.7287\n",
            "Epoch 528/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.5394 - val_loss: 100.4570\n",
            "Epoch 529/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 177.6467 - val_loss: 97.1239\n",
            "Epoch 530/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.4670 - val_loss: 97.1468\n",
            "Epoch 531/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.9239 - val_loss: 93.0071\n",
            "Epoch 532/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.5077 - val_loss: 95.6665\n",
            "Epoch 533/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 174.6353 - val_loss: 94.6773\n",
            "Epoch 534/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 174.6880 - val_loss: 97.6298\n",
            "Epoch 535/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.0501 - val_loss: 97.9983\n",
            "Epoch 536/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.5422 - val_loss: 96.4128\n",
            "Epoch 537/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.3612 - val_loss: 93.5522\n",
            "Epoch 538/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 169.8873 - val_loss: 95.7698\n",
            "Epoch 539/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.7686 - val_loss: 96.7835\n",
            "Epoch 540/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.0828 - val_loss: 98.3647\n",
            "Epoch 541/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 171.5545 - val_loss: 99.5689\n",
            "Epoch 542/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.2629 - val_loss: 98.2142\n",
            "Epoch 543/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.0164 - val_loss: 95.2196\n",
            "Epoch 544/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 180.0421 - val_loss: 95.4194\n",
            "Epoch 545/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.9415 - val_loss: 97.2699\n",
            "Epoch 546/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.5291 - val_loss: 97.4037\n",
            "Epoch 547/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 171.0350 - val_loss: 98.9341\n",
            "Epoch 548/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 168.8487 - val_loss: 98.3453\n",
            "Epoch 549/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 169.8892 - val_loss: 96.9622\n",
            "Epoch 550/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.0799 - val_loss: 97.6240\n",
            "Epoch 551/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.5353 - val_loss: 98.4849\n",
            "Epoch 552/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.2906 - val_loss: 96.0410\n",
            "Epoch 553/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 173.7945 - val_loss: 94.6569\n",
            "Epoch 554/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.9725 - val_loss: 98.3604\n",
            "Epoch 555/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 172.0310 - val_loss: 99.8033\n",
            "Epoch 556/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 173.0931 - val_loss: 100.8727\n",
            "Epoch 557/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 165.9216 - val_loss: 96.2756\n",
            "Epoch 558/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 168.9107 - val_loss: 98.9833\n",
            "Epoch 559/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 169.4258 - val_loss: 98.9615\n",
            "Epoch 560/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 162.2908 - val_loss: 99.7653\n",
            "Epoch 561/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.6652 - val_loss: 98.6621\n",
            "Epoch 562/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.9678 - val_loss: 95.9282\n",
            "Epoch 563/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 167.1539 - val_loss: 99.5698\n",
            "Epoch 564/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.3091 - val_loss: 98.4527\n",
            "Epoch 565/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 176.2564 - val_loss: 96.1692\n",
            "Epoch 566/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.6742 - val_loss: 98.9885\n",
            "Epoch 567/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.0478 - val_loss: 101.0339\n",
            "Epoch 568/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.9242 - val_loss: 97.3227\n",
            "Epoch 569/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 164.4027 - val_loss: 99.8373\n",
            "Epoch 570/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 170.4065 - val_loss: 97.1344\n",
            "Epoch 571/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 167.2503 - val_loss: 95.8042\n",
            "Epoch 572/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 165.3925 - val_loss: 95.4462\n",
            "Epoch 573/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.0650 - val_loss: 96.6129\n",
            "Epoch 574/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 164.4214 - val_loss: 94.4079\n",
            "Epoch 575/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.9530 - val_loss: 99.6891\n",
            "Epoch 576/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.4413 - val_loss: 103.4799\n",
            "Epoch 577/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.8783 - val_loss: 100.2301\n",
            "Epoch 578/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 164.5839 - val_loss: 98.4203\n",
            "Epoch 579/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 168.5541 - val_loss: 101.6616\n",
            "Epoch 580/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 164.6225 - val_loss: 97.8980\n",
            "Epoch 581/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.4543 - val_loss: 98.6749\n",
            "Epoch 582/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 172.1658 - val_loss: 96.8306\n",
            "Epoch 583/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 166.0089 - val_loss: 100.8094\n",
            "Epoch 584/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.0891 - val_loss: 99.1073\n",
            "Epoch 585/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.3983 - val_loss: 100.4034\n",
            "Epoch 586/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.4105 - val_loss: 99.8454\n",
            "Epoch 587/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 169.6880 - val_loss: 98.5479\n",
            "Epoch 588/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 170.6138 - val_loss: 99.8021\n",
            "Epoch 589/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 160.4409 - val_loss: 98.3391\n",
            "Epoch 590/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 171.6700 - val_loss: 99.5295\n",
            "Epoch 591/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.7233 - val_loss: 105.9759\n",
            "Epoch 592/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 175.6496 - val_loss: 100.9960\n",
            "Epoch 593/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.2307 - val_loss: 100.7296\n",
            "Epoch 594/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.9922 - val_loss: 100.6515\n",
            "Epoch 595/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.0074 - val_loss: 100.2719\n",
            "Epoch 596/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.2204 - val_loss: 102.3679\n",
            "Epoch 597/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.3232 - val_loss: 96.4107\n",
            "Epoch 598/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.6772 - val_loss: 98.1879\n",
            "Epoch 599/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 164.7972 - val_loss: 100.3414\n",
            "Epoch 600/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.3897 - val_loss: 97.3370\n",
            "Epoch 601/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.5654 - val_loss: 98.6509\n",
            "Epoch 602/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.0594 - val_loss: 97.7165\n",
            "Epoch 603/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.5273 - val_loss: 100.4604\n",
            "Epoch 604/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 173.6731 - val_loss: 102.4163\n",
            "Epoch 605/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.7645 - val_loss: 103.6297\n",
            "Epoch 606/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.2462 - val_loss: 100.3832\n",
            "Epoch 607/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.3759 - val_loss: 98.9729\n",
            "Epoch 608/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.7492 - val_loss: 99.4411\n",
            "Epoch 609/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.7692 - val_loss: 101.2255\n",
            "Epoch 610/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.2644 - val_loss: 98.2577\n",
            "Epoch 611/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.8514 - val_loss: 100.3558\n",
            "Epoch 612/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.4509 - val_loss: 98.2908\n",
            "Epoch 613/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.5399 - val_loss: 101.0089\n",
            "Epoch 614/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 166.4981 - val_loss: 105.7164\n",
            "Epoch 615/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 165.9006 - val_loss: 100.9183\n",
            "Epoch 616/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 158.3172 - val_loss: 101.3259\n",
            "Epoch 617/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.8275 - val_loss: 95.9788\n",
            "Epoch 618/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.6299 - val_loss: 98.4152\n",
            "Epoch 619/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.8729 - val_loss: 96.0353\n",
            "Epoch 620/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 158.9900 - val_loss: 101.0452\n",
            "Epoch 621/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.5188 - val_loss: 96.0621\n",
            "Epoch 622/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.8226 - val_loss: 98.6535\n",
            "Epoch 623/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.5651 - val_loss: 97.9444\n",
            "Epoch 624/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 159.9531 - val_loss: 94.7047\n",
            "Epoch 625/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.7425 - val_loss: 96.9768\n",
            "Epoch 626/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.8245 - val_loss: 95.0840\n",
            "Epoch 627/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.0630 - val_loss: 96.3663\n",
            "Epoch 628/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.2157 - val_loss: 94.0255\n",
            "Epoch 629/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.5609 - val_loss: 97.1069\n",
            "Epoch 630/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 164.2076 - val_loss: 96.9029\n",
            "Epoch 631/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 163.1049 - val_loss: 96.5200\n",
            "Epoch 632/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.7108 - val_loss: 99.9385\n",
            "Epoch 633/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 158.2673 - val_loss: 98.9128\n",
            "Epoch 634/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 161.4966 - val_loss: 99.9942\n",
            "Epoch 635/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.0555 - val_loss: 97.8675\n",
            "Epoch 636/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 167.0634 - val_loss: 98.7228\n",
            "Epoch 637/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.3339 - val_loss: 97.7272\n",
            "Epoch 638/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 157.7919 - val_loss: 99.0741\n",
            "Epoch 639/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 158.2286 - val_loss: 98.3300\n",
            "Epoch 640/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 158.8928 - val_loss: 99.6310\n",
            "Epoch 641/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 159.3011 - val_loss: 97.7735\n",
            "Epoch 642/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.3830 - val_loss: 96.8392\n",
            "Epoch 643/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.0524 - val_loss: 97.0550\n",
            "Epoch 644/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.2418 - val_loss: 98.3448\n",
            "Epoch 645/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 161.8260 - val_loss: 96.7540\n",
            "Epoch 646/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.9158 - val_loss: 95.5114\n",
            "Epoch 647/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.1931 - val_loss: 97.1525\n",
            "Epoch 648/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.8853 - val_loss: 96.2206\n",
            "Epoch 649/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 158.4405 - val_loss: 96.3108\n",
            "Epoch 650/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 157.7974 - val_loss: 98.4144\n",
            "Epoch 651/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.5321 - val_loss: 99.0415\n",
            "Epoch 652/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.2046 - val_loss: 98.4321\n",
            "Epoch 653/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.6062 - val_loss: 97.5142\n",
            "Epoch 654/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 155.1012 - val_loss: 97.5254\n",
            "Epoch 655/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 160.1680 - val_loss: 98.5528\n",
            "Epoch 656/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.0260 - val_loss: 98.7282\n",
            "Epoch 657/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 158.6244 - val_loss: 97.6817\n",
            "Epoch 658/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 162.5595 - val_loss: 98.4930\n",
            "Epoch 659/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.9158 - val_loss: 100.1776\n",
            "Epoch 660/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.3306 - val_loss: 99.3472\n",
            "Epoch 661/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 158.4392 - val_loss: 97.6970\n",
            "Epoch 662/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 152.2730 - val_loss: 99.7004\n",
            "Epoch 663/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.7782 - val_loss: 95.9179\n",
            "Epoch 664/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.2246 - val_loss: 95.9417\n",
            "Epoch 665/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 156.0635 - val_loss: 96.1502\n",
            "Epoch 666/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 155.9621 - val_loss: 101.3592\n",
            "Epoch 667/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.5917 - val_loss: 101.0519\n",
            "Epoch 668/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.6333 - val_loss: 99.4776\n",
            "Epoch 669/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 162.4947 - val_loss: 95.5154\n",
            "Epoch 670/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 157.1897 - val_loss: 97.5826\n",
            "Epoch 671/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 158.2677 - val_loss: 97.5648\n",
            "Epoch 672/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 160.6822 - val_loss: 93.6250\n",
            "Epoch 673/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.8048 - val_loss: 95.1206\n",
            "Epoch 674/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.8089 - val_loss: 96.2579\n",
            "Epoch 675/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 154.9350 - val_loss: 93.5683\n",
            "Epoch 676/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.4073 - val_loss: 98.2848\n",
            "Epoch 677/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 157.4844 - val_loss: 97.9471\n",
            "Epoch 678/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.3871 - val_loss: 96.5790\n",
            "Epoch 679/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.9547 - val_loss: 99.9864\n",
            "Epoch 680/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.6819 - val_loss: 98.9747\n",
            "Epoch 681/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.7479 - val_loss: 97.0220\n",
            "Epoch 682/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.8742 - val_loss: 98.9774\n",
            "Epoch 683/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.2095 - val_loss: 97.3258\n",
            "Epoch 684/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.0760 - val_loss: 99.6516\n",
            "Epoch 685/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.2734 - val_loss: 96.8029\n",
            "Epoch 686/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 152.6174 - val_loss: 99.9009\n",
            "Epoch 687/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.3445 - val_loss: 99.9431\n",
            "Epoch 688/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.7616 - val_loss: 98.1012\n",
            "Epoch 689/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 151.1332 - val_loss: 99.5051\n",
            "Epoch 690/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.1195 - val_loss: 98.2328\n",
            "Epoch 691/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.5713 - val_loss: 99.1341\n",
            "Epoch 692/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 170.6573 - val_loss: 97.1431\n",
            "Epoch 693/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 158.1486 - val_loss: 104.0024\n",
            "Epoch 694/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 151.9825 - val_loss: 101.5927\n",
            "Epoch 695/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 151.9067 - val_loss: 98.7877\n",
            "Epoch 696/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 151.6494 - val_loss: 103.1237\n",
            "Epoch 697/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 159.3985 - val_loss: 98.2182\n",
            "Epoch 698/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 151.0393 - val_loss: 98.0411\n",
            "Epoch 699/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 152.7290 - val_loss: 100.3904\n",
            "Epoch 700/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.4186 - val_loss: 101.9689\n",
            "Epoch 701/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 152.9436 - val_loss: 96.3264\n",
            "Epoch 702/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.7735 - val_loss: 97.0819\n",
            "Epoch 703/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.0683 - val_loss: 96.8183\n",
            "Epoch 704/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.2963 - val_loss: 97.0841\n",
            "Epoch 705/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.9247 - val_loss: 100.6019\n",
            "Epoch 706/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 154.3037 - val_loss: 95.5673\n",
            "Epoch 707/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.1731 - val_loss: 93.5632\n",
            "Epoch 708/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 151.6194 - val_loss: 97.9563\n",
            "Epoch 709/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 152.8389 - val_loss: 96.9481\n",
            "Epoch 710/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.7852 - val_loss: 94.0798\n",
            "Epoch 711/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.1236 - val_loss: 95.9315\n",
            "Epoch 712/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.3754 - val_loss: 96.6899\n",
            "Epoch 713/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.9213 - val_loss: 96.0789\n",
            "Epoch 714/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.0928 - val_loss: 94.9639\n",
            "Epoch 715/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.5057 - val_loss: 95.0276\n",
            "Epoch 716/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 152.1274 - val_loss: 95.9055\n",
            "Epoch 717/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.5699 - val_loss: 97.3185\n",
            "Epoch 718/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.9386 - val_loss: 99.0459\n",
            "Epoch 719/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 149.5210 - val_loss: 94.6669\n",
            "Epoch 720/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 149.5520 - val_loss: 95.1949\n",
            "Epoch 721/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 149.1801 - val_loss: 97.1643\n",
            "Epoch 722/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 149.3946 - val_loss: 95.0371\n",
            "Epoch 723/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 151.0555 - val_loss: 94.7985\n",
            "Epoch 724/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 156.2914 - val_loss: 97.8524\n",
            "Epoch 725/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.2541 - val_loss: 98.9642\n",
            "Epoch 726/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.7098 - val_loss: 97.4863\n",
            "Epoch 727/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 151.7068 - val_loss: 99.6410\n",
            "Epoch 728/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 146.9579 - val_loss: 99.0973\n",
            "Epoch 729/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 155.7295 - val_loss: 101.2608\n",
            "Epoch 730/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.6600 - val_loss: 98.3330\n",
            "Epoch 731/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 153.6537 - val_loss: 98.5140\n",
            "Epoch 732/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.1306 - val_loss: 97.9176\n",
            "Epoch 733/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 152.2431 - val_loss: 97.4480\n",
            "Epoch 734/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 144.1950 - val_loss: 98.7856\n",
            "Epoch 735/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.6750 - val_loss: 99.8569\n",
            "Epoch 736/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 148.6911 - val_loss: 101.9323\n",
            "Epoch 737/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 152.5543 - val_loss: 96.6397\n",
            "Epoch 738/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 153.3749 - val_loss: 99.2288\n",
            "Epoch 739/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 148.2278 - val_loss: 100.7653\n",
            "Epoch 740/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.8142 - val_loss: 100.1686\n",
            "Epoch 741/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.3454 - val_loss: 99.0338\n",
            "Epoch 742/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 149.2281 - val_loss: 99.2747\n",
            "Epoch 743/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.5180 - val_loss: 96.6227\n",
            "Epoch 744/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.6328 - val_loss: 98.6982\n",
            "Epoch 745/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.4764 - val_loss: 100.8439\n",
            "Epoch 746/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 146.4226 - val_loss: 96.7094\n",
            "Epoch 747/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 148.0421 - val_loss: 95.4220\n",
            "Epoch 748/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.2698 - val_loss: 96.6224\n",
            "Epoch 749/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.3652 - val_loss: 94.9668\n",
            "Epoch 750/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.3094 - val_loss: 96.9089\n",
            "Epoch 751/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.4941 - val_loss: 94.3270\n",
            "Epoch 752/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 146.6928 - val_loss: 95.3770\n",
            "Epoch 753/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 143.4528 - val_loss: 96.7101\n",
            "Epoch 754/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.5155 - val_loss: 95.8814\n",
            "Epoch 755/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 148.1330 - val_loss: 94.9504\n",
            "Epoch 756/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 148.6060 - val_loss: 95.3960\n",
            "Epoch 757/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.9737 - val_loss: 98.7282\n",
            "Epoch 758/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 148.8444 - val_loss: 99.1361\n",
            "Epoch 759/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 151.4720 - val_loss: 95.7526\n",
            "Epoch 760/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 144.0768 - val_loss: 99.0012\n",
            "Epoch 761/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 165.6325 - val_loss: 99.3596\n",
            "Epoch 762/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.7224 - val_loss: 100.6455\n",
            "Epoch 763/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.8715 - val_loss: 97.3204\n",
            "Epoch 764/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 148.9503 - val_loss: 97.0802\n",
            "Epoch 765/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 150.1872 - val_loss: 95.4609\n",
            "Epoch 766/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.6464 - val_loss: 94.6121\n",
            "Epoch 767/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.2843 - val_loss: 100.8208\n",
            "Epoch 768/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.9612 - val_loss: 100.5063\n",
            "Epoch 769/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 150.6058 - val_loss: 100.2614\n",
            "Epoch 770/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.8275 - val_loss: 98.7389\n",
            "Epoch 771/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 143.9547 - val_loss: 97.8207\n",
            "Epoch 772/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 145.6651 - val_loss: 99.5013\n",
            "Epoch 773/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 144.4280 - val_loss: 99.6397\n",
            "Epoch 774/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.8623 - val_loss: 95.8354\n",
            "Epoch 775/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 146.5464 - val_loss: 96.4389\n",
            "Epoch 776/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 146.7157 - val_loss: 101.2923\n",
            "Epoch 777/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 143.9974 - val_loss: 96.9807\n",
            "Epoch 778/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.3524 - val_loss: 95.1029\n",
            "Epoch 779/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.3535 - val_loss: 95.0606\n",
            "Epoch 780/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 146.0024 - val_loss: 96.6720\n",
            "Epoch 781/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.9237 - val_loss: 96.1646\n",
            "Epoch 782/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 146.7967 - val_loss: 96.3461\n",
            "Epoch 783/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 142.7104 - val_loss: 100.0098\n",
            "Epoch 784/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.0281 - val_loss: 98.0422\n",
            "Epoch 785/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.6557 - val_loss: 101.3302\n",
            "Epoch 786/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.4225 - val_loss: 98.8795\n",
            "Epoch 787/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.5264 - val_loss: 95.8270\n",
            "Epoch 788/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.3473 - val_loss: 99.4651\n",
            "Epoch 789/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 146.1831 - val_loss: 98.1702\n",
            "Epoch 790/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.5290 - val_loss: 98.2482\n",
            "Epoch 791/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 144.9753 - val_loss: 97.3408\n",
            "Epoch 792/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 146.0824 - val_loss: 96.1676\n",
            "Epoch 793/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 144.1550 - val_loss: 96.5833\n",
            "Epoch 794/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 144.3903 - val_loss: 94.5245\n",
            "Epoch 795/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 143.7191 - val_loss: 97.9756\n",
            "Epoch 796/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 143.3653 - val_loss: 98.7247\n",
            "Epoch 797/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.4482 - val_loss: 97.6806\n",
            "Epoch 798/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.3144 - val_loss: 98.2421\n",
            "Epoch 799/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.1316 - val_loss: 98.8847\n",
            "Epoch 800/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 142.4729 - val_loss: 99.0272\n",
            "Epoch 801/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 144.6709 - val_loss: 97.1383\n",
            "Epoch 802/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 144.3062 - val_loss: 99.6996\n",
            "Epoch 803/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 144.6332 - val_loss: 101.2122\n",
            "Epoch 804/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 147.9418 - val_loss: 96.9754\n",
            "Epoch 805/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.4242 - val_loss: 99.9328\n",
            "Epoch 806/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.3376 - val_loss: 98.0759\n",
            "Epoch 807/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.6479 - val_loss: 97.7852\n",
            "Epoch 808/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.8259 - val_loss: 97.4631\n",
            "Epoch 809/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.6524 - val_loss: 98.0730\n",
            "Epoch 810/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 147.4282 - val_loss: 98.8345\n",
            "Epoch 811/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.8726 - val_loss: 98.4418\n",
            "Epoch 812/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.6146 - val_loss: 101.6377\n",
            "Epoch 813/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.2668 - val_loss: 101.9406\n",
            "Epoch 814/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.3576 - val_loss: 100.7932\n",
            "Epoch 815/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.5712 - val_loss: 98.7839\n",
            "Epoch 816/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.8040 - val_loss: 96.9045\n",
            "Epoch 817/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.7488 - val_loss: 97.5131\n",
            "Epoch 818/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 142.7085 - val_loss: 98.7862\n",
            "Epoch 819/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 138.1145 - val_loss: 99.5402\n",
            "Epoch 820/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 140.4101 - val_loss: 97.3907\n",
            "Epoch 821/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.4413 - val_loss: 97.3145\n",
            "Epoch 822/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.3506 - val_loss: 96.9348\n",
            "Epoch 823/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 142.3913 - val_loss: 98.2971\n",
            "Epoch 824/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 142.5528 - val_loss: 97.0461\n",
            "Epoch 825/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 138.0171 - val_loss: 98.0065\n",
            "Epoch 826/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 141.3500 - val_loss: 99.7526\n",
            "Epoch 827/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.4560 - val_loss: 96.2238\n",
            "Epoch 828/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.9556 - val_loss: 97.7421\n",
            "Epoch 829/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.9452 - val_loss: 99.7733\n",
            "Epoch 830/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 139.6792 - val_loss: 97.0492\n",
            "Epoch 831/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.0918 - val_loss: 97.5033\n",
            "Epoch 832/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.0317 - val_loss: 98.6868\n",
            "Epoch 833/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 139.3841 - val_loss: 95.7902\n",
            "Epoch 834/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.9635 - val_loss: 97.5872\n",
            "Epoch 835/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 141.1203 - val_loss: 99.1015\n",
            "Epoch 836/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.1125 - val_loss: 97.8579\n",
            "Epoch 837/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.1844 - val_loss: 98.3366\n",
            "Epoch 838/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 140.6682 - val_loss: 96.4181\n",
            "Epoch 839/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.9236 - val_loss: 97.8804\n",
            "Epoch 840/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.1579 - val_loss: 98.1954\n",
            "Epoch 841/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.5562 - val_loss: 97.8468\n",
            "Epoch 842/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 136.1675 - val_loss: 99.1046\n",
            "Epoch 843/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.5244 - val_loss: 99.0509\n",
            "Epoch 844/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.2872 - val_loss: 98.6815\n",
            "Epoch 845/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.6506 - val_loss: 98.2491\n",
            "Epoch 846/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.5515 - val_loss: 98.7268\n",
            "Epoch 847/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 140.2635 - val_loss: 99.6449\n",
            "Epoch 848/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 141.2980 - val_loss: 97.8919\n",
            "Epoch 849/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.3295 - val_loss: 100.0498\n",
            "Epoch 850/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 148.0116 - val_loss: 98.9107\n",
            "Epoch 851/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 137.7559 - val_loss: 99.1960\n",
            "Epoch 852/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 140.7864 - val_loss: 97.4028\n",
            "Epoch 853/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.7627 - val_loss: 97.6004\n",
            "Epoch 854/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.9435 - val_loss: 99.4482\n",
            "Epoch 855/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.3293 - val_loss: 98.1502\n",
            "Epoch 856/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 139.6419 - val_loss: 99.2978\n",
            "Epoch 857/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 138.8807 - val_loss: 99.2220\n",
            "Epoch 858/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 140.7054 - val_loss: 95.7596\n",
            "Epoch 859/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 141.3652 - val_loss: 95.4175\n",
            "Epoch 860/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.4148 - val_loss: 98.2478\n",
            "Epoch 861/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.1187 - val_loss: 103.3426\n",
            "Epoch 862/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.5693 - val_loss: 101.0690\n",
            "Epoch 863/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.5384 - val_loss: 98.2841\n",
            "Epoch 864/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 139.2808 - val_loss: 101.0585\n",
            "Epoch 865/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.1748 - val_loss: 101.1586\n",
            "Epoch 866/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 140.3476 - val_loss: 99.9461\n",
            "Epoch 867/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 137.4543 - val_loss: 100.1686\n",
            "Epoch 868/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.5316 - val_loss: 98.6158\n",
            "Epoch 869/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.1109 - val_loss: 98.1853\n",
            "Epoch 870/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.8693 - val_loss: 98.7552\n",
            "Epoch 871/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.2317 - val_loss: 97.8121\n",
            "Epoch 872/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 139.6147 - val_loss: 96.9639\n",
            "Epoch 873/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.1299 - val_loss: 98.3618\n",
            "Epoch 874/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 137.3895 - val_loss: 98.2011\n",
            "Epoch 875/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.5128 - val_loss: 95.7797\n",
            "Epoch 876/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 138.4050 - val_loss: 97.5162\n",
            "Epoch 877/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 135.7048 - val_loss: 98.7979\n",
            "Epoch 878/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.3449 - val_loss: 97.4065\n",
            "Epoch 879/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 135.9489 - val_loss: 94.0261\n",
            "Epoch 880/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.9652 - val_loss: 96.1538\n",
            "Epoch 881/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.0775 - val_loss: 95.6012\n",
            "Epoch 882/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.4925 - val_loss: 97.3890\n",
            "Epoch 883/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.6430 - val_loss: 95.6591\n",
            "Epoch 884/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 133.9379 - val_loss: 94.2891\n",
            "Epoch 885/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 136.1536 - val_loss: 97.1566\n",
            "Epoch 886/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.8403 - val_loss: 95.2050\n",
            "Epoch 887/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.6583 - val_loss: 96.7812\n",
            "Epoch 888/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.1974 - val_loss: 95.0790\n",
            "Epoch 889/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 133.2633 - val_loss: 95.2884\n",
            "Epoch 890/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 135.4713 - val_loss: 95.1888\n",
            "Epoch 891/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 132.3795 - val_loss: 94.2603\n",
            "Epoch 892/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.4216 - val_loss: 95.8960\n",
            "Epoch 893/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.2514 - val_loss: 96.2338\n",
            "Epoch 894/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.6879 - val_loss: 98.2820\n",
            "Epoch 895/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.7525 - val_loss: 96.3042\n",
            "Epoch 896/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 135.4239 - val_loss: 96.8982\n",
            "Epoch 897/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 137.5697 - val_loss: 96.6887\n",
            "Epoch 898/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.5403 - val_loss: 95.6822\n",
            "Epoch 899/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 135.1743 - val_loss: 97.3962\n",
            "Epoch 900/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.7526 - val_loss: 96.2920\n",
            "Epoch 901/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.2125 - val_loss: 93.8853\n",
            "Epoch 902/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.6828 - val_loss: 95.2435\n",
            "Epoch 903/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.9439 - val_loss: 94.6843\n",
            "Epoch 904/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.6958 - val_loss: 92.9843\n",
            "Epoch 905/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.4060 - val_loss: 94.8145\n",
            "Epoch 906/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.0521 - val_loss: 95.1402\n",
            "Epoch 907/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 145.9643 - val_loss: 95.8424\n",
            "Epoch 908/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.9142 - val_loss: 96.1305\n",
            "Epoch 909/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 131.9819 - val_loss: 95.0970\n",
            "Epoch 910/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.8738 - val_loss: 96.2746\n",
            "Epoch 911/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.5908 - val_loss: 95.9638\n",
            "Epoch 912/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.5034 - val_loss: 95.2810\n",
            "Epoch 913/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.8882 - val_loss: 96.2740\n",
            "Epoch 914/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.9401 - val_loss: 96.1355\n",
            "Epoch 915/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.4476 - val_loss: 96.3956\n",
            "Epoch 916/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.1320 - val_loss: 96.8688\n",
            "Epoch 917/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.4504 - val_loss: 94.9153\n",
            "Epoch 918/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.9474 - val_loss: 94.5001\n",
            "Epoch 919/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 133.7551 - val_loss: 96.0395\n",
            "Epoch 920/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.1442 - val_loss: 95.3004\n",
            "Epoch 921/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 132.8792 - val_loss: 94.2534\n",
            "Epoch 922/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.0768 - val_loss: 96.0880\n",
            "Epoch 923/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.7108 - val_loss: 99.1868\n",
            "Epoch 924/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 133.0131 - val_loss: 95.8262\n",
            "Epoch 925/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.6256 - val_loss: 96.2290\n",
            "Epoch 926/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.4782 - val_loss: 94.3601\n",
            "Epoch 927/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 148.0411 - val_loss: 95.5055\n",
            "Epoch 928/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 136.1548 - val_loss: 97.2241\n",
            "Epoch 929/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.3858 - val_loss: 97.9153\n",
            "Epoch 930/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.7600 - val_loss: 99.0562\n",
            "Epoch 931/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.0994 - val_loss: 97.8919\n",
            "Epoch 932/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 133.7776 - val_loss: 98.1096\n",
            "Epoch 933/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.5558 - val_loss: 96.5364\n",
            "Epoch 934/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.5537 - val_loss: 95.4760\n",
            "Epoch 935/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.5002 - val_loss: 97.7959\n",
            "Epoch 936/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 135.9590 - val_loss: 96.0333\n",
            "Epoch 937/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.1934 - val_loss: 96.1456\n",
            "Epoch 938/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.6809 - val_loss: 99.1158\n",
            "Epoch 939/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 136.3440 - val_loss: 95.8981\n",
            "Epoch 940/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.9512 - val_loss: 99.6438\n",
            "Epoch 941/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.1002 - val_loss: 95.7913\n",
            "Epoch 942/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.2511 - val_loss: 96.5362\n",
            "Epoch 943/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.0692 - val_loss: 97.5560\n",
            "Epoch 944/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.8310 - val_loss: 98.3257\n",
            "Epoch 945/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.7560 - val_loss: 94.1471\n",
            "Epoch 946/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.9475 - val_loss: 96.3797\n",
            "Epoch 947/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.9140 - val_loss: 95.6253\n",
            "Epoch 948/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 133.5721 - val_loss: 96.1480\n",
            "Epoch 949/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.6601 - val_loss: 95.5726\n",
            "Epoch 950/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.9676 - val_loss: 98.1700\n",
            "Epoch 951/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 143.7414 - val_loss: 95.5087\n",
            "Epoch 952/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 134.3375 - val_loss: 94.6640\n",
            "Epoch 953/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.9494 - val_loss: 94.8587\n",
            "Epoch 954/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.9467 - val_loss: 97.3561\n",
            "Epoch 955/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.5682 - val_loss: 95.1549\n",
            "Epoch 956/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 127.3839 - val_loss: 95.5259\n",
            "Epoch 957/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.8288 - val_loss: 96.8018\n",
            "Epoch 958/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.2297 - val_loss: 96.1377\n",
            "Epoch 959/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.7807 - val_loss: 96.0566\n",
            "Epoch 960/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.0145 - val_loss: 95.1845\n",
            "Epoch 961/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 128.6463 - val_loss: 95.5583\n",
            "Epoch 962/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.7619 - val_loss: 97.4159\n",
            "Epoch 963/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.0180 - val_loss: 95.8619\n",
            "Epoch 964/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 129.9067 - val_loss: 95.6395\n",
            "Epoch 965/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.0712 - val_loss: 95.9493\n",
            "Epoch 966/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 129.8668 - val_loss: 96.0055\n",
            "Epoch 967/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.9388 - val_loss: 95.8712\n",
            "Epoch 968/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 129.9179 - val_loss: 97.7390\n",
            "Epoch 969/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.0549 - val_loss: 96.3676\n",
            "Epoch 970/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.5472 - val_loss: 103.2570\n",
            "Epoch 971/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.0691 - val_loss: 96.9032\n",
            "Epoch 972/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.4517 - val_loss: 98.8551\n",
            "Epoch 973/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.3331 - val_loss: 99.1667\n",
            "Epoch 974/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.0629 - val_loss: 100.3218\n",
            "Epoch 975/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.5992 - val_loss: 99.2226\n",
            "Epoch 976/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.4093 - val_loss: 98.5927\n",
            "Epoch 977/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.4646 - val_loss: 100.0557\n",
            "Epoch 978/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 130.5475 - val_loss: 100.6208\n",
            "Epoch 979/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 132.5597 - val_loss: 98.8820\n",
            "Epoch 980/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 132.0381 - val_loss: 98.3503\n",
            "Epoch 981/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 128.5863 - val_loss: 100.7309\n",
            "Epoch 982/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 128.8886 - val_loss: 100.9824\n",
            "Epoch 983/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.4998 - val_loss: 99.2204\n",
            "Epoch 984/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 127.4281 - val_loss: 98.2182\n",
            "Epoch 985/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 128.8684 - val_loss: 95.0558\n",
            "Epoch 986/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 127.6956 - val_loss: 97.7074\n",
            "Epoch 987/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.0205 - val_loss: 95.6914\n",
            "Epoch 988/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.2451 - val_loss: 96.1055\n",
            "Epoch 989/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 129.4541 - val_loss: 93.8376\n",
            "Epoch 990/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 134.5120 - val_loss: 95.2805\n",
            "Epoch 991/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 128.0462 - val_loss: 94.6201\n",
            "Epoch 992/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 131.1449 - val_loss: 94.6098\n",
            "Epoch 993/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.6712 - val_loss: 96.0491\n",
            "Epoch 994/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 129.3239 - val_loss: 93.0226\n",
            "Epoch 995/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 126.7462 - val_loss: 95.3787\n",
            "Epoch 996/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 128.6528 - val_loss: 94.3222\n",
            "Epoch 997/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 127.0554 - val_loss: 96.3855\n",
            "Epoch 998/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 131.9570 - val_loss: 97.5878\n",
            "Epoch 999/1000\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 138.7681 - val_loss: 99.1270\n",
            "Epoch 1000/1000\n",
            "68/68 [==============================] - 0s 4ms/step - loss: 130.8071 - val_loss: 96.2638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "OLVJ3pBhinxX",
        "outputId": "eb3f3bed-e62a-46e5-8180-1e60febd75d4"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJVdL54njGyw",
        "outputId": "1c19fd15-8db3-45ce-d0f4-9060ffbeba21"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A74KPJ8pOSR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}